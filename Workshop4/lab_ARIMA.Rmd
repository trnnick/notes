---
title: 'ARIMA modelling in R'
author: "Nikolaos Kourentzes (<nikolaos@kourentes.com>)"
output:
  pdf_document: 
    toc: true
    number_sections: true
    toc_depth: 2
    fig_width: 6
    fig_height: 4.5
    fig_caption: false
  word_document: default
  html_document: default
geometry: margin=0.75in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data and packages

We will use two example time series, one from base R and one from the *forecast* package. There are a few different alternatives for modelling ARIMA with R. The first is the `arima` function that comes built-in with R. This function provides the core for doing manual modelling with ARIMA; but requires the modeller to identify the appropriate model. More advanced alternatives exist in the *forecast* and *smooth* packages. The first extends the `arima` function to include seasonality and automatic or semi-automatic model identification. The second offers a new implementation from scratch while providing both automatic and detailed manual modelling.

Let us load the *forecast* package. 
```{r, results = 'hide'}
library(forecast)
```

It is important to remember that the data need to be presented as time series to the functions. You can check is something is a time series using the function `class`. For example:

```{r}
class(AirPassengers)
```

Observe that the result is `ts`, which is a shorthand for time series. These objects contain additional information about the sampling frequency of the data and the start/end period. Observe:

```{r}
AirPassengers
```

# Data exploration
We will use the `AirPassengers` time series that comes with R. It is always a good idea to first plot the time series!

```{r}
plot(AirPassengers)
```

We note that it is dominated by a seasonal pattern and an increasing trend. 

To avoid typing the name of the times series all the time, but also to allow you to run the same analysis with different data, we will store the series in a variable called `y`.

```{r}
y <- AirPassengers
```

To get the ACF and the PACF plots we can use the functions `acf`and `pacf` respectively. 

```{r}
acf(y)
pacf(y)
```

I do not want to spend much time on these plots. Even though many textbooks insist that these are somehow useful, as we demonstrated earlier in the lecture, they quickly become useless for anything more complex than clean AR or MA processes of low order, for e.g., AR(1) or AR(2). Even the original inventors of ACF and PACF have stepped away from these now that we have increased computational and statistical power!

A more useful function is `diff` that helps us introduce differencing to remove non-stationarities. These may appear as a lack of a global mean, for instance, a stochastic trend, or a stochastic seasonality. The term stochastic suggests that the process is driven by the error term of the model, which itself is a random variable. Practically, we can explain stochastic as a trend or a seasonality that is evolving over time, in contrast to deterministic that remains constant. Observe the plot of the time series before. The seasonality evolves over time. 

> Note that here we avoid going into the technicalities of stationary time series, but it is important to note that we should not mix the existence of trend or seasonality with the definition of stationarity. It just happens that, for instance, the trend is also non-stationary. The inverse does not always hold! This is because the term trend is not uniquely defined across modelling classes. 

Back to our example! Differencing can be in levels or seasonal. Observe the effect:

```{r}
yD <- diff(y) # Differencing in levels, also called first differencing
plot(yD)
```

The seasonal shape is still visible, but the trend is removed, hence the term differences in level. Intuitively we are now modelling the rate of change of the time series. 

```{r}
# Seasonal differencing. This is a monthly time series, 
# and 12 corresponds to the seasonal periodicity.
yDs <- diff(y,12) 
plot(yDs)
```

Now the seasonal component is removed and the remaining information appears to follow an erratic (read as stochastic!) trend. We can of course apply both sets of differencing. 

```{r}
yDDs <- diff(diff(y,12)) 
plot(yDDs)
```

Which seems to be free of both seasonal and trend elements. Now the series is stationary and we can look for AR or MA components relatively easy. 

> Plot the ACF and PACF for different versions of differencing and observe how the plots become more informative as the time series becomes stationary. Consider the following, if a time series has a trend then the previous observation is highly correlated with the current (co-moving to the direction of the trend), so does with the observation two-periods ago and so on. This contaminates the ACF and PACF making them harder to read and use.

We can apply multiple times either level or seasonal differencing. This is to deal with higher *order* of non-stationary. This is rarely needed in practice, and even if it is we very rarely consider more than a second set of differencing. Intuitively that would be the same as modelling the rate of change of the rate of change. For instance, this may appear when there are apparent non-linearities in how the trend shifts over time, i.e., its rate of change is itself non-stationary! For 99.99% of the cases you will be okay with trying a single level and/or seasonal difference.

If you are finding that both seasonality and trend may need differencing, it is good practice to first consider seasonal differences, as these may be enough to deal with the trend as well. 

Finally, note that the differencing is the integration (I) term in ARIMA. 

# Building an ARIMA model

## Differencing

In most cases building an ARIMA can be seen as a two-step process. First, we identify the order of differencing and make our time series stationary, and second, we identify the appropriate order of the AR and MA terms. For the first step, we rely on time series exploration (or statistical tests if we need to automate). For the second step, we can use information criteria, such as the Akaike Information Criterion (AIC).

In R there are functions that implement many of the popular statistical tests to identify the order of differencing. For seasonal differences a helpful function is `nsdiffs`. The function implements four common statistical tests (type `?nsdiffs` to see the details in the help file), and also tries for higher order of differencing. It will return a numerical value which is what should be the order of seasonal differencing, i.e., SARIMA(x,x,x)(x,this number,x).

```{r}
nsdiffs(y)
```

Here the test suggests we should apply seasonal differences of order 1.

Similarly, for level differences we can use the function `ndiffs` that works in a similar fashion. We test of level differences after applying any seasonal differences. We have already calculated `yDs`, so we will use that.

```{r}
ndiffs(yDs)
```

Again we first the first order level differences are sufficient. That would be in an ARIMA(x,this number,x) or in a SARIMA(x,this number,x)(x,x,x). As it happens we tried these options already, so the variable `yDDs` is what we need, which we had already plotted and found to appear stationary. 

If any of the tests would give you a number higher than 1, then you would just apply the differencing operator again.

## Modelling of the ARMA parts

Once we have made our time series stationary, we can proceed with the main part of the modelling. First, we should get familiar with the functions. We will use the `Arima` function from the *forecast* package. It is worthwhile to see all the options that it offers by typing ´?Arima´. The three first arguments are the important ones: (i) the time series; (ii) the ARIMA orders, as AR, I, MA; and (iii) the seasonal ARIMA orders, again in the same order. If the second or the third term are left blank, then they are assumed to be (0,0,0).

For example, we fit an ARMA(1,0) to our stationary series. Observe that I dropped the differencing part, as I have already made my time series stationary. 

```{r}
Arima(yDDs,order=c(1,0,0))
```

I could also ask the function to do that for me. This is better, as it greatly simplifies the creation of forecast, so that I do not have to reverse the differencing manually. Observe that the results are the same if I type:

```{r}
Arima(y,order=c(1,1,0),seasonal=c(0,1,0))
```

I used series `y` and include the differencing in both parts of the SARIMA. We will always use `y` directly from now on. At this point, I can try many different models to my data, with various alternatives for AR and MA, seasonal or not. I need some way to quickly compare the models. To do this we can rely on information criteria, such as the AICc. The AICc is a "corrected" form of AIC for small sample sizes, a common problem in many forecasting applications. For long time series AICc and AIC are almost identical. 

First, we can produce a few models that we can compare. I will rely on lists and for-loops to do this quickly. If you are unfamiliar with lists, these are very flexible memory objects in R that you can use to store any type of information. 

```{r}
# Initialise a variable to store the results
models <- list()
# For now, I will not deal with the seasonal part.
# I will instead try to find models with AR from 0 to 3,
# and MA from 0 to 3. To do this I will use two loops, 
# one for the AR and one for the MA. 
AR <- MA <- 0:3
k <- 1 # A simple index to help me go through the list

# The loops start here
for (i in 1:length(AR)){ # length(AR) gives me the size of AR variable
  for (j in 1:length(MA)){
    models[[k]] <- Arima(y,order=c(i,1,j),seasonal=c(0,1,0))
    # Note that with lists we use double square brackets [[ ]]
    k <- k + 1 # This simple index makes my life easy with the list
  }
}
```

This should produce 16 models (4 AR options times 4 MA options). We can inspect any model we want by calling it from the list. For example:
```{r}
models[[10]]
```

As you can see we get all the coefficients (and their estimation errors), as well as various information criteria. In principle we could use the coefficients and their standard error and calculate p-values for these and test for significance, potentially simplifying the model. This has been one standard approach for model identification with ARIMA (or regression more generally), but it has quite a few caveats. It is largely considered to be superseded by using information criteria instead. Note that information criteria and p-values do not play well together. Information criteria may prefer a model that has insignificant terms, and vice versa. There is good theory in favour of information criteria, so we will do that. This is also one of the reasons that R does not output p-values readily! 

The AICc values are stored in the models, so we can ask for that information:
```{r}
models[[10]]$aicc
```

If you compare with the output above, and you will verify that the value is indeed the reported AICc. In case you are wondering what else is stored in that list, you can use `names(models[[10]])`. You will see that you can extract most elements that make up the model. 

We used lists so that we can get all this information with as few lines of code as possible, so to get all AICc's we can just ask:
```{r}
AICc <- sapply(models,function(x){x$aicc})
AICc
```

The function `sapply` goes through all elements of the list and applies the function that follows. There I just ask it to get the AICc value. We have a vector of AICc that we can compare to find the best model (smallest AICc). We can do that very easily with the `which.min` function.

```{r}
which.min(AICc)
```

It seems that model 11 is the best from all the alternatives we tried.
```{r}
selModel <- models[[11]]
selModel
```

Naturally, this is the best from the 16 alternatives we tried and not generally. None of these 16 models explored the seasonal ARMA. You can quickly see that it can be very easy to get lost in searching across for different parameters for the non-seasonal/seasonal AR and MA. In the literature there are many smart heuristics for how to search the model space. Some rely on integer optimisation, while others are closer to the ideas of stepwise regression (iteratively expand or restrict your model). The function `auto.arima` does exactly that. It uses a smart heuristic to find reasonable orders for the model. It should be noted that it does not try all combinations! That can easily become a massive computational burden when we have to move across 4 directions of search.

```{r}
autoModel <- auto.arima(y)
autoModel
```

As it happens it comes up with a slightly simpler model than our selected one. It seems to be content with no seasonal ARMA terms. Also observe that its AICc is slightly larger, suggesting our model may be better. If you look at the help of `auto.arima` (using ? before the function's name, as before) you will observe the argument `ic` that instructs the heuristic what information criteria to optimise. As it happens this is the AICc. The reason that we disagree in the model is because we did an exhaustive search. The function provides a result faster and also evaluated some seasonal ARMA, so it had to cut corners somewhere! Another detail is that the orders of differencing were identified using the same functions we used. In fact this is again applied sequentially, first we test for stationarity, difference, and then deal with the ARMA terms. 

*Important!* Note that we cannot use information criteria to compare models that do not have the same differencing! The reason for that is in the calculation of information criteria we use the log-likelihood of the model, which is only comparable when the data are identical. Differencing will change the sample size (necessarily we lose the very first data points to construct our differences), and the scale of the data (both the mean and the variance will change with differencing). That is a limitation of conventional ARIMA modelling. If we embed ARIMA within a state-space formulation, then we can use information criteria, as differencing is handled differently. This is what the ARIMA implementation in the *smooth* package does! If you refer to my slides you will observe that some ETS models imply various orders of differencing that would normally not be comparable using information criteria. The reason we can do that is again due to the state-space formulation. 

For this example, we prefer our model that is stored in `selModel`. The function in R will give you a warning if the model is invalid, so checking the coefficients manually is not necessary. Nonetheless, it is easy to plot them:
```{r}
plot(selModel)
```

For the model to be valid all *inverse* roots must be within the unit circle. The plots show this both for the AR and MA terms, ensuring that the model behaves as it should!

To illustrate the difficulty of reading the ACF and PACF for real data, we use the stationary version of the series:
```{r}
acf(yDDs)
pacf(yDDs)
```

To me, it is not apparent how these charts suggest the selected model!

## Forecasting

Now that we have identified a reasonable model we can construct forecasts. Forecasts for ARIMA are constructed iteratively, that is we forecast t+1, and then use it to forecast t+2, and so on. Thankfully this is handled by the software. So is differencing. If we did everything manually then we would have to reverse any differencing once we have constructed our forecasts. This is not necessary with the existing functions, as long as the differencing is part of the model and not done externally. 

To produce forecasts we use the `forecast` function that takes two arguments, first the model and then the forecast horizon. 

```{r}
frc <- forecast(selModel,h=24)
frc
```

This gives us automatically the point forecast (forecasted mean) as well as the 80% and 95% prediction intervals. We can specify other intervals with additional arguments in the function (see help!). It is perhaps easier to understand the output if we visualise the forecasts.

```{r}
plot(frc)
```

Not a terrible job! Note that the seasonal pattern was handled by the seasonal differencing and no additional seasonal AR or MA terms were needed. 

We saved the forecasts in the variable `frc` that is a list. We use `names` to see what it contains
```{r}
names(frc)
```

There are quite a few variables therein. I draw your attention to `mean` that contains the point forecasts, the `lower` and `upper` that contains the lower and upper prediction intervals, and `fitted` that contains the in-sample model fit. For example, we can extract the point forecasts as:

```{r}
frc$mean
```

Observe that the time series characteristics (sampling frequency and dates) are carried from the original `y`. ARIMA needs `y` to be a time series so that it can know what seasonal periodicity to use, if any, and to annotate dates correctly. 

> In principle there is nothing to prohibit us from introducing multiple seasonalities with ARIMA, for instance with hourly data you may observe an hour of the day pattern, a day of the week pattern, and a day in the year pattern. We would simply include additional seasonal ARIMA components. Although in principle this is very simple, in practice the identification of the correct model becomes very difficult. Currently, the functions discussed here do not support this either. However, one could use differencing as an easy approximate fix. We would preprocess the data to remove two seasonalities through respective differencing, and then model the rest with a single seasonal ARIMA. 

## Practice

Use the time series `wineind` to practice your arima skills! This time series is the total bottle sales of Australian wine. 

```{r}
plot(wineind)
```

**Happy forecasting!**